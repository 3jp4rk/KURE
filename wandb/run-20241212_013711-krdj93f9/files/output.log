  0%|                                                                               | 0/15 [00:00<?, ?it/s]/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2906: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
Traceback (most recent call last):
  File "/home/ubuntu/ejpark/koe5_train/train.py", line 136, in <module>
    main()
  File "/home/ubuntu/ejpark/koe5_train/train.py", line 131, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/transformers/trainer.py", line 1938, in train
    return inner_training_loop(
  File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/transformers/trainer.py", line 2279, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/transformers/trainer.py", line 3318, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ubuntu/ejpark/koe5_train/train.py", line 34, in compute_loss
    output = model(**input_dict)
  File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 834, in forward
    encoder_outputs = self.encoder(
  File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 522, in forward
    layer_outputs = layer_module(
  File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 411, in forward
    self_attention_outputs = self.attention(
  File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 338, in forward
    self_outputs = self.self(
  File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 227, in forward
    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 79.10 GiB of which 385.88 MiB is free. Including non-PyTorch memory, this process has 78.71 GiB memory in use. Of the allocated memory 75.03 GiB is allocated by PyTorch, and 313.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/ejpark/koe5_train/train.py", line 136, in <module>
[rank0]:     main()
[rank0]:   File "/home/ubuntu/ejpark/koe5_train/train.py", line 131, in main
[rank0]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)
[rank0]:   File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/transformers/trainer.py", line 1938, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/transformers/trainer.py", line 2279, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs)
[rank0]:   File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/transformers/trainer.py", line 3318, in training_step
[rank0]:     loss = self.compute_loss(model, inputs)
[rank0]:   File "/home/ubuntu/ejpark/koe5_train/train.py", line 34, in compute_loss
[rank0]:     output = model(**input_dict)
[rank0]:   File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank0]:   File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank0]:   File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 834, in forward
[rank0]:     encoder_outputs = self.encoder(
[rank0]:   File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 522, in forward
[rank0]:     layer_outputs = layer_module(
[rank0]:   File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 411, in forward
[rank0]:     self_attention_outputs = self.attention(
[rank0]:   File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 338, in forward
[rank0]:     self_outputs = self.self(
[rank0]:   File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/ubuntu/ejpark/embed_train/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py", line 227, in forward
[rank0]:     attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 79.10 GiB of which 385.88 MiB is free. Including non-PyTorch memory, this process has 78.71 GiB memory in use. Of the allocated memory 75.03 GiB is allocated by PyTorch, and 313.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
